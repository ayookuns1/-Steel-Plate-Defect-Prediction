{"cells":[{"cell_type":"markdown","metadata":{},"source":["**PROBLEM STATEMENT: Steel Plate Defect Prediction**\n","\n","<font size=\"3\">\n","A dataset of steel plate faults, classified into 7 different types. The goal was to train machine learning for automatic pattern recognition. </font>\n","\n","<font size=\"3\">Type of dependent variables (7 Types of Steel Plates Faults):</font>\n","\n","1. Pastry\n","\n","2. Z_Scratch\n","\n","3. K_Scatch\n","\n","4. Stains\n","\n","5. Dirtiness\n","\n","6. Bumps\n","\n","7. Other_Faults\n","\n","**EVALUTAION METRIC:** ROC AUC\n","\n","<font size=\"3\">There are no missing values in the data</font>"]},{"cell_type":"markdown","metadata":{},"source":["# 2. IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-03-03T06:43:33.07733Z","iopub.status.busy":"2024-03-03T06:43:33.076798Z","iopub.status.idle":"2024-03-03T06:46:22.189127Z","shell.execute_reply":"2024-03-03T06:46:22.187672Z","shell.execute_reply.started":"2024-03-03T06:43:33.077296Z"},"trusted":true},"outputs":[],"source":["import sklearn\n","import numpy as np\n","import os\n","import datetime\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import missingno as msno\n","from prettytable import PrettyTable\n","%matplotlib inline\n","import seaborn as sns\n","sns.set(style='darkgrid', font_scale=1.4)\n","from tqdm import tqdm\n","from tqdm.notebook import tqdm as tqdm_notebook\n","tqdm_notebook.get_lock().locks = []\n","# !pip install sweetviz\n","# import sweetviz as sv\n","import concurrent.futures\n","from copy import deepcopy       \n","from functools import partial\n","from itertools import combinations\n","import random\n","from random import randint, uniform\n","import gc\n","from sklearn.feature_selection import f_classif\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n","from sklearn import metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","from itertools import combinations\n","from sklearn.impute import SimpleImputer\n","import xgboost as xg\n","from sklearn.model_selection import train_test_split,cross_val_score\n","from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n","from sklearn.cluster import KMeans\n","!pip install yellowbrick\n","from yellowbrick.cluster import KElbowVisualizer\n","!pip install gap-stat\n","from gap_statistic.optimalK import OptimalK\n","from scipy import stats\n","import statsmodels.api as sm\n","from scipy.stats import ttest_ind\n","from scipy.stats import boxcox\n","import math\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from sklearn.base import BaseEstimator, TransformerMixin\n","!pip install optuna\n","import optuna\n","!pip install cmaes\n","import cmaes\n","import xgboost as xgb\n","!pip install catboost\n","!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n","import lightgbm as lgb\n","!pip install category_encoders\n","from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n","!pip install -U imbalanced-learn\n","from imblearn.ensemble import BalancedRandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n","from sklearn.svm import NuSVC, SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.impute import KNNImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from catboost import Pool\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import TruncatedSVD\n","\n","# Suppress warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","pd.pandas.set_option('display.max_columns',None)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.1 LOAD DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-03T06:46:22.193949Z","iopub.status.busy":"2024-03-03T06:46:22.191666Z","iopub.status.idle":"2024-03-03T06:46:22.516905Z","shell.execute_reply":"2024-03-03T06:46:22.515619Z","shell.execute_reply.started":"2024-03-03T06:46:22.193899Z"},"trusted":true},"outputs":[],"source":["train=pd.read_csv('/kaggle/input/SteelData/train.csv')\n","test=pd.read_csv('/kaggle/input/SteelData/test.csv')\n","original=pd.read_csv(\"/kaggle/input/steel-plates-faults/SteelPlatesFaults.csv\")\n","\n","train.drop(columns=[\"id\"],inplace=True)\n","test.drop(columns=[\"id\"],inplace=True)\n","\n","train_copy=train.copy()\n","test_copy=test.copy()\n","original_copy=original.copy()\n","\n","print(original.shape)\n","\n","device='cpu'\n","\n","train=pd.concat([train,original],axis=0)\n","train.reset_index(inplace=True,drop=True)\n","\n","target=['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n","\n","original.head()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. EXPLORATORY DATA ANALYSIS"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 Numerical Feature Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-03-03T06:46:22.519113Z","iopub.status.busy":"2024-03-03T06:46:22.518666Z"},"trusted":true},"outputs":[],"source":["cont_cols = test.columns\n","colors = ['blue', 'orange', 'green']  \n","\n","num_plots = len(cont_cols)\n","num_cols = 3  \n","num_rows = -(-num_plots // num_cols)  \n","fig, axes = plt.subplots(num_rows, num_cols, figsize=(21, 5 * num_rows))  # Adjust the figure size as needed\n","\n","for i, feature in enumerate(cont_cols):\n","    row = i // num_cols\n","    col = i % num_cols\n","\n","    ax = axes[row, col] if num_rows > 1 else axes[col]\n","    \n","    sns.histplot(train_copy[feature], kde=True, color=colors[0], label='Train', alpha=0.5, bins=30, ax=ax)\n","    sns.histplot(test_copy[feature], kde=True, color=colors[1], label='Test', alpha=0.5, bins=30, ax=ax)\n","    sns.histplot(original[feature], kde=True, color=colors[2], label='Original', alpha=0.5, bins=30, ax=ax)\n","\n","    ax.set_title(f'Distribution of {feature}')\n","    ax.set_xlabel(feature)\n","    ax.set_ylabel('Frequency')\n","    ax.legend()\n","\n","if num_plots % num_cols != 0:\n","    for j in range(num_plots % num_cols, num_cols):\n","        axes[-1, j].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\"> Some features have a lot of outliers and few features seem to have dual distributions</font>"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def OHE(train_df,test_df,cols,target):\n","    '''\n","    Function for one hot encoding, it first combined the data so that no category is missed and\n","    the category with least frequency can be dropped because of redunancy\n","    '''\n","    combined = pd.concat([train_df, test_df], axis=0)\n","    for col in cols:\n","        one_hot = pd.get_dummies(combined[col]).astype(int)\n","        counts = combined[col].value_counts()\n","        min_count_category = counts.idxmin()\n","        one_hot = one_hot.drop(min_count_category, axis=1)\n","        one_hot.columns=[str(f)+col for f in one_hot.columns]\n","        combined = pd.concat([combined, one_hot], axis=\"columns\")\n","        combined = combined.loc[:, ~combined.columns.duplicated()]\n","    \n","    # split back to train and test dataframes\n","    train_ohe = combined[:len(train_df)]\n","    test_ohe = combined[len(train_df):]\n","    test_ohe.reset_index(inplace=True,drop=True)\n","    test_ohe.drop(columns=[target],inplace=True)\n","    return train_ohe, test_ohe"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1 Stabilize Categorical Columns"]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\">I have converted the descrete features with unique less than 5% to categorical</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.idle":"2024-03-03T06:47:24.552731Z","shell.execute_reply":"2024-03-03T06:47:24.551631Z","shell.execute_reply.started":"2024-03-03T06:46:52.628013Z"},"trusted":true},"outputs":[],"source":["cat_cols = [f for f in test.columns if test[f].nunique()/test.shape[0]*100<5 and test[f].nunique()>2 ]\n","test[cat_cols].nunique()\n","\n","def nearest_val(target):\n","    return min(common, key=lambda x: abs(x - target))\n","\n","global cat_cols_updated\n","cat_cols_updated=[]\n","for col in cat_cols:\n","    train[f\"{col}_cat\"]=train[col]\n","    test[f\"{col}_cat\"]=test[col]\n","    cat_cols_updated.append(f\"{col}_cat\")\n","    uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n","    if uncommon:\n","        common=list(set(test[col].unique())& set(train[col].unique()))\n","        train[f\"{col}_cat\"]=train[col].apply(nearest_val)\n","        test[f\"{col}_cat\"]=test[col].apply(nearest_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:16.486324Z","iopub.status.busy":"2024-03-03T07:22:16.485868Z","iopub.status.idle":"2024-03-03T07:22:16.516857Z","shell.execute_reply":"2024-03-03T07:22:16.515606Z","shell.execute_reply.started":"2024-03-03T07:22:16.486291Z"},"trusted":true},"outputs":[],"source":["def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n","    '''\n","    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied, \n","    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n","    '''\n","    train_copy=train.copy()\n","    test_copy=test.copy()\n","    ohe_cols=[]\n","    for col in extra_cols:\n","        dict1=train_copy[col].value_counts().to_dict()\n","        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n","        rare_keys=list([*ordered.keys()][n_limit:])\n","#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n","        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n","        \n","        train_copy[col]=train_copy[col].replace(rare_key_map)\n","        test_copy[col]=test_copy[col].replace(rare_key_map)\n","    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n","    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n","    train_copy=train_copy.drop(columns=drop_cols)\n","    test_copy=test_copy.drop(columns=drop_cols)\n","    \n","    return train_copy, test_copy\n","\n","def cat_encoding(train, test, target):\n","    global overall_best_score\n","    global overall_best_col\n","    table = PrettyTable()\n","    table.field_names = ['Feature', 'Encoded Features', 'Log Loss Score']\n","    train_copy=train.copy()\n","    test_copy=test.copy()\n","    train_dum = train.copy()\n","    for feature in cat_cols_updated:\n","#         print(feature)\n","#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n","#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n","#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n","#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n","\n","        dic = train[feature].value_counts().to_dict()\n","        train_copy[feature + \"_count\"] =train[feature].map(dic)\n","        test_copy[feature + \"_count\"] = test[feature].map(dic)\n","\n","        dic2=train[feature].value_counts().to_dict()\n","#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n","        list1=np.arange(len(dic2.values())) # Higher rank for low count\n","        dic3=dict(zip(list(dic2.keys()),list1))\n","        \n","        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n","        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n","\n","        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#feature + \"_target\",\n","\n","        \n","\n","        if train_copy[feature].nunique()<=5:\n","            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n","            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n","            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n","            \n","        else:\n","            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=5)\n","        train_copy=train_copy.drop(columns=[feature])\n","        test_copy=test_copy.drop(columns=[feature])\n","        \n","        \n","        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","        auc_scores = []\n","\n","        for f in temp_cols:\n","            X = train_copy[[f]].values\n","            y = train_copy[target].astype(int).values\n","\n","            auc = []\n","            for train_idx, val_idx in kf.split(X, y):\n","                X_train, y_train = X[train_idx], y[train_idx]\n","                x_val, y_val = X[val_idx], y[val_idx]\n","                model =  HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n","                model.fit(X_train, y_train)\n","                y_pred = model.predict_proba(x_val)[:,1]\n","                auc.append(roc_auc_score(y_val,  y_pred))\n","            auc_scores.append((f, np.mean(auc)))\n","            \n","        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n","\n","        corr = train_copy[temp_cols].corr(method='pearson')\n","        corr_with_best_col = corr[best_col]\n","        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n","        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n","        if cols_to_drop:\n","            train_copy = train_copy.drop(columns=cols_to_drop)\n","            test_copy = test_copy.drop(columns=cols_to_drop)\n","\n","        table.add_row([feature, best_col, best_auc])\n","#         print(feature)\n","        \n","#     print(table)\n","    return train_copy, test_copy"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Modeling"]},{"cell_type":"markdown","metadata":{},"source":["## 5.1 Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:17.236284Z","iopub.status.busy":"2024-03-03T07:22:17.235849Z","iopub.status.idle":"2024-03-03T07:22:17.270072Z","shell.execute_reply":"2024-03-03T07:22:17.26862Z","shell.execute_reply.started":"2024-03-03T07:22:17.236251Z"},"trusted":true},"outputs":[],"source":["class Splitter:\n","    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n","        self.test_size = test_size\n","        self.kfold = kfold\n","        self.n_splits = n_splits\n","\n","    def split_data(self, X, y, random_state_list):\n","        if self.kfold:\n","            for random_state in random_state_list:\n","                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n","                for train_index, val_index in kf.split(X, y):\n","                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","                    yield X_train, X_val, y_train, y_val\n","\n","class Classifier:\n","    def __init__(self,n_estimators=100, device=\"cpu\", random_state=0):\n","        self.n_estimators = n_estimators\n","        self.device = device\n","        self.random_state = random_state\n","        self.models = self._define_model()\n","        self.len_models = len(self.models)\n","        \n","    def _define_model(self):\n","        xgb_params = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.1,\n","            'max_depth': 4,\n","            'subsample': 0.8,\n","            'colsample_bytree': 0.1,\n","            'n_jobs': -1,\n","            'eval_metric': 'logloss',\n","            'objective': 'binary:logistic',\n","            'tree_method': 'hist',\n","            'verbosity': 0,\n","            'random_state': self.random_state,\n","#             'class_weight':class_weights_dict,\n","        }\n","        if self.device == 'gpu':\n","            xgb_params['tree_method'] = 'gpu_hist'\n","            xgb_params['predictor'] = 'gpu_predictor'\n","            \n","        xgb_params2=xgb_params.copy() \n","        xgb_params2['subsample']= 0.5\n","        xgb_params2['max_depth']=9\n","        xgb_params2['learning_rate']=0.045\n","        xgb_params2['colsample_bytree']=0.3\n","\n","        xgb_params3=xgb_params.copy() \n","        xgb_params3['subsample']= 0.6\n","        xgb_params3['max_depth']=6\n","        xgb_params3['learning_rate']=0.02\n","        xgb_params3['colsample_bytree']=0.7      \n","\n","        xgb_params4=xgb_params.copy() \n","        xgb_params4['subsample']= 0.5943421542786502\n","        xgb_params4['max_depth']=6\n","        xgb_params4['learning_rate']=0.109\n","        xgb_params4['colsample_bytree']=0.5595039093313848\n","        lgb_params = {\n","            'n_estimators': self.n_estimators,\n","            'max_depth': 8,\n","            'learning_rate': 0.02,\n","            'subsample': 0.20,\n","            'colsample_bytree': 0.56,\n","            'reg_alpha': 0.25,\n","            'reg_lambda': 5e-08,\n","            'objective': 'binary',\n","            'boosting_type': 'gbdt',\n","            'device': self.device,\n","            'random_state': self.random_state,\n","            'verbose':-1,\n","#             'class_weight':class_weights_dict,\n","        } \n","        lgb_params2 = {\n","            'n_estimators': self.n_estimators,\n","            'max_depth': 5,\n","            'learning_rate': 0.015,\n","            'subsample': 0.50,\n","            'colsample_bytree': 0.1,\n","            'reg_alpha': 0.07608657669988828,\n","            'reg_lambda': 0.2255036530113883,\n","            'objective': 'binary',\n","            'boosting_type': 'gbdt',\n","            'device': self.device,\n","            'random_state': self.random_state,\n","        }\n","        lgb_params3=lgb_params.copy()  \n","        lgb_params3['subsample']=0.9\n","        lgb_params3['reg_lambda']=0.3461495211744402\n","        lgb_params3['reg_alpha']=0.3095626288582237\n","        lgb_params3['max_depth']=8\n","        lgb_params3['learning_rate']=0.007\n","        lgb_params3['colsample_bytree']=0.5\n","\n","        lgb_params4=lgb_params2.copy()  \n","        lgb_params4['subsample']=0.3\n","        lgb_params4['reg_lambda']=0.49406951573373614\n","        lgb_params4['reg_alpha']=0.16269100796945424\n","        lgb_params4['max_depth']=9\n","        lgb_params4['learning_rate']=0.117\n","        lgb_params4['colsample_bytree']=0.3\n","\n","        cb_params = {\n","            'iterations': self.n_estimators,\n","            'depth': 13,\n","            'learning_rate': 0.015,\n","            'l2_leaf_reg': 0.5,\n","            'random_strength': 0.1,\n","            'max_bin': 200,\n","            'od_wait': 65,\n","            'one_hot_max_size': 50,\n","            'grow_policy': 'Depthwise',\n","            'bootstrap_type': 'Bernoulli',\n","            'od_type': 'Iter',\n","            'eval_metric': 'AUC',\n","            'loss_function': 'Logloss',\n","            'task_type': self.device.upper(),\n","            'random_state': self.random_state,\n","        }\n","        cb_sym_params = cb_params.copy()\n","        cb_sym_params['grow_policy'] = 'SymmetricTree'\n","        cb_loss_params = cb_params.copy()\n","        cb_loss_params['grow_policy'] = 'Lossguide'\n","        \n","        cb_params2=  cb_params.copy()\n","        cb_params2['learning_rate']=0.01\n","        cb_params2['depth']=8\n","\n","        cb_params3={\n","            'iterations': self.n_estimators,\n","            'random_strength': 0.5783342241486167, \n","            'one_hot_max_size': 10, \n","            'max_bin': 150, \n","            'learning_rate': 0.177, \n","            'l2_leaf_reg': 0.705662073971363, \n","            'grow_policy': 'SymmetricTree', \n","            'depth': 5, \n","            'max_bin': 200,\n","            'od_wait': 65,\n","            'bootstrap_type': 'Bayesian',\n","            'od_type': 'Iter',\n","            'eval_metric': 'AUC',\n","            'loss_function': 'Logloss',\n","            'task_type': self.device.upper(),\n","            'random_state': self.random_state,\n","        }\n","        cb_params4=  cb_params.copy()\n","        cb_params4['learning_rate']=0.01\n","        cb_params4['depth']=12\n","        dt_params= {'min_samples_split': 30, 'min_samples_leaf': 10, 'max_depth': 8, 'criterion': 'gini'}\n","        \n","        models = {\n","            'xgb': xgb.XGBClassifier(**xgb_params),\n","#            'xgb2': xgb.XGBClassifier(**xgb_params2),\n","#            'xgb3': xgb.XGBClassifier(**xgb_params3),\n","#            'xgb4': xgb.XGBClassifier(**xgb_params4),\n","#            'lgb': lgb.LGBMClassifier(**lgb_params),\n","#             'lgb2': lgb.LGBMClassifier(**lgb_params2),\n","#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n","#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n","            'cat': CatBoostClassifier(**cb_params),\n","#            'cat2': CatBoostClassifier(**cb_params2),\n","#             'cat3': CatBoostClassifier(**cb_params3),\n","#             'cat4': CatBoostClassifier(**cb_params4),\n","             \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n","#             \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n","#             'hist_gbm' : HistGradientBoostingClassifier (max_iter=300, learning_rate=0.001,  max_leaf_nodes=80,                                                       \n","#            max_depth=6,random_state=self.random_state),#class_weight=class_weights_dict, \n","#             'gbdt': GradientBoostingClassifier(max_depth=6,  n_estimators=1000,random_state=self.random_state),\n","#             'lr': LogisticRegression(),\n","#             'rf': RandomForestClassifier(max_depth= 9,max_features= 'auto',min_samples_split= 10,\n","#                                                           min_samples_leaf= 4,  n_estimators=500,random_state=self.random_state),\n","#            'svc': SVC(gamma=\"auto\", probability=True),\n","#             'knn': KNeighborsClassifier(n_neighbors=5),\n","#             'mlp': MLPClassifier(random_state=self.random_state, max_iter=1000),\n","#             'etr':ExtraTreesClassifier(min_samples_split=55, min_samples_leaf= 15, max_depth=10,\n","#                                        n_estimators=200,random_state=self.random_state),\n","#             'dt' :DecisionTreeClassifier(**dt_params,random_state=self.random_state),\n","#             'ada': AdaBoostClassifier(random_state=self.random_state),\n","                                       \n","        }\n","        return models"]},{"cell_type":"markdown","metadata":{},"source":["## 5.2 Optimize Ensemble Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:17.760208Z","iopub.status.busy":"2024-03-03T07:22:17.759753Z","iopub.status.idle":"2024-03-03T07:22:17.775176Z","shell.execute_reply":"2024-03-03T07:22:17.77386Z","shell.execute_reply.started":"2024-03-03T07:22:17.760172Z"},"trusted":true},"outputs":[],"source":["class OptunaWeights:\n","    def __init__(self, random_state, n_trials=5000):\n","        self.study = None\n","        self.weights = None\n","        self.random_state = random_state\n","        self.n_trials = n_trials\n","\n","    def _objective(self, trial, y_true, y_preds):\n","        # Define the weights for the predictions from each model\n","        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n","\n","        # Calculate the weighted prediction\n","        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n","\n","        auc_score = roc_auc_score(y_true, weighted_pred)\n","        log_loss_score=log_loss(y_true, weighted_pred)\n","        return auc_score#/log_loss_score\n","\n","    def fit(self, y_true, y_preds):\n","        optuna.logging.set_verbosity(optuna.logging.ERROR)\n","        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n","        pruner = optuna.pruners.HyperbandPruner()\n","        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n","        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n","        self.study.optimize(objective_partial, n_trials=self.n_trials)\n","        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n","\n","    def predict(self, y_preds):\n","        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n","        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n","        return weighted_pred\n","\n","    def fit_predict(self, y_true, y_preds):\n","        self.fit(y_true, y_preds)\n","        return self.predict(y_preds)\n","    \n","    def weights(self):\n","        return self.weights"]},{"cell_type":"markdown","metadata":{},"source":["## 5.3 Fit The Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:18.292044Z","iopub.status.busy":"2024-03-03T07:22:18.29121Z","iopub.status.idle":"2024-03-03T07:22:18.313414Z","shell.execute_reply":"2024-03-03T07:22:18.311792Z","shell.execute_reply.started":"2024-03-03T07:22:18.292002Z"},"trusted":true},"outputs":[],"source":["def fit_model(X_train,X_test,y_train):\n","    kfold = True\n","    n_splits = 1 if not kfold else 5\n","    random_state = 2023\n","    random_state_list = [42] # used by split_data [71]\n","    n_estimators = 9999 # 9999\n","    early_stopping_rounds = 300\n","    verbose = False\n","\n","    splitter = Splitter(kfold=kfold, n_splits=n_splits)\n","\n","    # Initialize an array for storing test predictions\n","    test_predss = np.zeros(X_test.shape[0])\n","    y_train_pred=y_train.copy()\n","\n","    ensemble_score = []\n","    weights = []\n","    trained_models = {'xgb':[], 'lgb':[]}\n","\n","\n","    for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n","        n = i % n_splits\n","        m = i // n_splits\n","\n","        # Get a set of Regressor models\n","        classifier = Classifier(n_estimators, device, random_state)\n","        models = classifier.models\n","\n","        # Initialize lists to store oof and test predictions for each base model\n","        oof_preds = []\n","        test_preds = []\n","\n","        # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n","        for name, model in models.items():\n","            if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n","                if 'lgb' in name: #categorical_feature=cat_features\n","                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)]),#,categorical_feature=cat_features,)\n","                elif 'cat' in name:\n","                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n","                              early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n","                else:\n","                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=0)\n","            else:\n","                model.fit(X_train_, y_train_)\n","\n","            test_pred = model.predict_proba(X_test)[:, 1]\n","            y_val_pred = model.predict_proba(X_val)[:, 1]\n","\n","            score = roc_auc_score(y_val, y_val_pred.reshape(-1, 1))\n","    #         score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n","\n","            print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] ROC AUC score: {score:.5f}')\n","\n","            oof_preds.append(y_val_pred)\n","            test_preds.append(test_pred)\n","\n","            if name in trained_models.keys():\n","                trained_models[f'{name}'].append(deepcopy(model))\n","        # Use Optuna to find the best ensemble weights\n","        optweights = OptunaWeights(random_state=random_state)\n","        y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n","\n","        score = roc_auc_score(y_val, y_val_pred.reshape(-1, 1))\n","    #     score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n","        print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ------------------>  ROC AUC score {score:.5f}')\n","        ensemble_score.append(score)\n","        weights.append(optweights.weights)\n","\n","        test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n","        y_train_pred.loc[y_val.index]=np.array(y_val_pred)\n","\n","        gc.collect()\n","    # Calculate the mean ROC AUC  score of the ensemble\n","    mean_score = np.mean(ensemble_score)\n","    std_score = np.std(ensemble_score)\n","    print(f'Ensemble ROC AUC score {mean_score:.5f} ± {std_score:.5f}')\n","\n","    # Print the mean and standard deviation of the ensemble weights for each model\n","    print('--- Model Weights ---')\n","    mean_weights = np.mean(weights, axis=0)\n","    std_weights = np.std(weights, axis=0)\n","    for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n","        print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')\n","    print(f'Overall OOF Preds AUC SCORE {roc_auc_score(y_train,y_train_pred)}')\n","    \n","    print(\"__________________________________________________________________\")\n","    return test_predss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:18.524883Z","iopub.status.busy":"2024-03-03T07:22:18.524472Z","iopub.status.idle":"2024-03-03T07:22:18.533378Z","shell.execute_reply":"2024-03-03T07:22:18.53207Z","shell.execute_reply.started":"2024-03-03T07:22:18.524853Z"},"trusted":true},"outputs":[],"source":["def post_processor(train, test):\n","    cols=test.columns.tolist()\n","    train_cop=train.copy()\n","    test_cop=test.copy()\n","    drop_cols=[]\n","    for i, feature in enumerate(cols):\n","        for j in range(i+1, len(cols)):\n","            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n","                if cols[j] not in drop_cols:\n","                    drop_cols.append(cols[j])\n","    print(drop_cols)\n","    train_cop.drop(columns=drop_cols,inplace=True)\n","    test_cop.drop(columns=drop_cols,inplace=True)\n","    \n","    return train_cop, test_cop\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:18.790641Z","iopub.status.busy":"2024-03-03T07:22:18.790211Z","iopub.status.idle":"2024-03-03T07:22:18.823915Z","shell.execute_reply":"2024-03-03T07:22:18.822884Z","shell.execute_reply.started":"2024-03-03T07:22:18.790608Z"},"trusted":true},"outputs":[],"source":["submission=pd.read_csv(\"/kaggle/input/playground-series-s4e3/sample_submission.csv\")\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T07:22:19.584185Z","iopub.status.busy":"2024-03-03T07:22:19.583771Z"},"trusted":true},"outputs":[],"source":["count=0\n","for col in target:\n","    train_temp=train[test.columns.tolist()+[col]]\n","    test_temp=test.copy()\n","    train_temp, test_temp= cat_encoding(train_temp, test_temp, col)\n","    \n","    final_features=test.columns.tolist()\n","    sc=StandardScaler()\n","\n","    train_scaled=train_temp.copy()\n","    test_scaled=test_temp.copy()\n","    \n","    train_scaled[final_features]=sc.fit_transform(train[final_features])\n","    test_scaled[final_features]=sc.transform(test[final_features])\n","\n","#     train_cop, test_cop=   post_processor(train_scaled, test_scaled) \n","    train_cop, test_cop= train_scaled, test_scaled\n","    X_train = train_cop.drop(columns=[col])\n","    y_train = train_cop[col]\n","    \n","    X_test = test_cop.copy()\n","    \n","    test_predss=fit_model(X_train,X_test,y_train)\n","    submission[col]=test_predss\n","    \n","    count+=1\n","    print(f'Column {col}, loop # {count}')\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7659021,"sourceId":68699,"sourceType":"competition"},{"datasetId":4518922,"sourceId":7733156,"sourceType":"datasetVersion"},{"sourceId":165700135,"sourceType":"kernelVersion"},{"sourceId":168014668,"sourceType":"kernelVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
